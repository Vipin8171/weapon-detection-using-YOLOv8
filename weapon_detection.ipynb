{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available:  True\n",
      "Num GPUs Available:  1\n",
      "GPU Name:  NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "PyTorch Version:  2.6.0+cu126\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(\"CUDA Available: \", cuda_available)\n",
    "\n",
    "# If CUDA is available, check the number of GPUs\n",
    "if cuda_available:\n",
    "    print(\"Num GPUs Available: \", torch.cuda.device_count())\n",
    "\n",
    "    # Print the name of the GPU\n",
    "    print(\"GPU Name: \", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"CUDA is not available. Please check your setup.\")\n",
    "\n",
    "# Print the PyTorch version\n",
    "print(\"PyTorch Version: \", torch.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E6j-MUx9bxRa"
   },
   "source": [
    "# DAtAset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-zNam-5ubvcd"
   },
   "source": [
    "Great! To enhance your weapon detection project, consider forking the following datasets from Roboflow Universe:\n",
    "\n",
    "1. **Weapon Detection by yolov7test**\n",
    "   - **Description:** Contains 9,672 images labeled with various weapon classes, including guns, knives, and rifles.\n",
    "   - **Link:** [Weapon Detection Dataset](https://universe.roboflow.com/yolov7test-u13vc/weapon-detection-m7qso)\n",
    "\n",
    "2. **Weapons Dataset**\n",
    "   - **Description:** Features images annotated with classes like grenades, guns, knives, and pistols.\n",
    "   - **Link:** [Weapons Dataset](https://universe.roboflow.com/weapons-dataset/weapons-dataset-os1ki)\n",
    "\n",
    "3. **Person and Weapon Detection**\n",
    "   - **Description:** Designed for security surveillance, this dataset includes images of individuals carrying weapons, aiding in real-time identification.\n",
    "   - **Link:** [Person and Weapon Detection Dataset](https://universe.roboflow.com/school-fin7c/person-weapon-datasets)\n",
    "\n",
    "4. **Weapon Detection by test**\n",
    "   - **Description:** Comprises 9,633 images with annotations for grenades, knives, missiles, pistols, and rifles.\n",
    "   - **Link:** [Weapon Detection Dataset](https://universe.roboflow.com/test-7awfy/weapon-detection-f1lih)\n",
    "\n",
    "5. **Weapon Detection Using YOLOv8**\n",
    "   - **Description:** Contains 671 images focusing on handguns, shotguns, knives, and rifles, suitable for training YOLOv8 models.\n",
    "   - **Link:** [Weapon Detection Using YOLOv8 Dataset](https://universe.roboflow.com/weopon-detection/weapon-detection-using-yolov8)\n",
    "\n",
    "Forking these datasets will provide a diverse set of images and annotations, enhancing your model's ability to detect various weapons in different scenarios.\n",
    "\n",
    "*Note: Always review the dataset's licensing and usage terms to ensure compliance with your project's requirements.*\n",
    "\n",
    "Let me know if you need assistance with the forking process or integrating these datasets into your project!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y4kEYaePb6ow"
   },
   "source": [
    "# I am using 5th one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yXZnBtP3yJKf"
   },
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zOy3nIgpyG9G"
   },
   "source": [
    "# **Weapon Detection using YOLO - Summary**\n",
    "\n",
    "Here's a structured **summary** of all the steps taken so far for **weapon detection using YOLO:**\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "## **1️⃣ Setting Up the Environment**  \n",
    "- Installed required dependencies (`ultralytics`, `opencv-python`).  \n",
    "- Used **Google Colab/Jupyter Notebook** for running the project.  \n",
    "\n",
    "## **2️⃣ Downloading the Dataset**  \n",
    "- Used **Roboflow API** to download a **weapon detection dataset**.  \n",
    "- Verified that the dataset contained **annotated images** for training.  \n",
    "\n",
    "## **3️⃣ Training the YOLO Model**  \n",
    "- Selected **YOLOv8** as the model architecture.  \n",
    "- Trained a **custom YOLOv8 model** (`best.pt`) using **Roboflow dataset**.  \n",
    "- Verified **training logs, validation metrics, and loss curves**.  \n",
    "\n",
    "## **4️⃣ Running Live Weapon Detection**  \n",
    "- Loaded **trained YOLO model** (`best.pt`) for inference.  \n",
    "- Opened **live webcam feed** using OpenCV.  \n",
    "- Applied **real-time object detection** on video frames.  \n",
    "- Drawn **bounding boxes** around detected weapons.  \n",
    "\n",
    "## **5️⃣ Improving Detection & Fixing Errors**  \n",
    "- Fixed **webcam errors** (`cv2.VideoCapture(0)` issues).  \n",
    "- Used `cv2.imshow()` for **real-time frame display**.  \n",
    "- Ensured **proper release of webcam** (`cap.release() & cv2.destroyAllWindows()`).  \n",
    "- **Increased detection accuracy** by setting a **confidence threshold** (`> 0.5`).  \n",
    "\n",
    "---\n",
    "\n",
    "✅ **Project Status:** **Working Real-Time Weapon Detection Model** 🚀  \n",
    "🔹 **Next Steps:** Improve dataset quality, train for more epochs, test in real-world environments.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NxqGIgXXb3Yr"
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "CVo1Ocr2XPMX",
    "outputId": "def46186-b7ff-4b72-c9f0-45e96667db07",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting ultralytics\n",
      "  Downloading ultralytics-8.3.107-py3-none-any.whl.metadata (37 kB)\n",
      "Collecting roboflow\n",
      "  Downloading roboflow-1.1.61-py3-none-any.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\tvipi\\anaconda3\\envs\\gpu_env\\lib\\site-packages (4.11.0.86)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\tvipi\\anaconda3\\envs\\gpu_env\\lib\\site-packages (3.10.1)\n",
      "Requirement already satisfied: torch in c:\\users\\tvipi\\anaconda3\\envs\\gpu_env\\lib\\site-packages (2.6.0+cu126)\n",
      "Requirement already satisfied: numpy<=2.1.1,>=1.23.0 in c:\\users\\tvipi\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from ultralytics) (1.26.4)\n",
      "Requirement already satisfied: pillow>=7.1.2 in c:\\users\\tvipi\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from ultralytics) (11.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in c:\\users\\tvipi\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from ultralytics) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.23.0 in c:\\users\\tvipi\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from ultralytics) (2.32.3)\n",
      "Requirement already satisfied: scipy>=1.4.1 in c:\\users\\tvipi\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from ultralytics) (1.15.2)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in c:\\users\\tvipi\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from ultralytics) (0.21.0+cu126)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in c:\\users\\tvipi\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from ultralytics) (4.67.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\tvipi\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from ultralytics) (7.0.0)\n",
      "Collecting py-cpuinfo (from ultralytics)\n",
      "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
      "Requirement already satisfied: pandas>=1.1.4 in c:\\users\\tvipi\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from ultralytics) (2.2.3)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in c:\\users\\tvipi\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from ultralytics) (0.13.2)\n",
      "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
      "  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: certifi in c:\\users\\tvipi\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from roboflow) (2025.1.31)\n",
      "Collecting idna==3.7 (from roboflow)\n",
      "  Downloading idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: cycler in c:\\users\\tvipi\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from roboflow) (0.12.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\tvipi\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from roboflow) (1.4.8)\n",
      "Collecting opencv-python-headless==4.10.0.84 (from roboflow)\n",
      "  Downloading opencv_python_headless-4.10.0.84-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Collecting pillow-heif>=0.18.0 (from roboflow)\n",
      "  Downloading pillow_heif-0.22.0-cp310-cp310-win_amd64.whl.metadata (9.8 kB)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\tvipi\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from roboflow) (2.9.0.post0)\n",
      "Collecting python-dotenv (from roboflow)\n",
      "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: six in c:\\users\\tvipi\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from roboflow) (1.17.0)\n",
      "Requirement already satisfied: urllib3>=1.26.6 in c:\\users\\tvipi\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from roboflow) (2.3.0)\n",
      "Collecting requests-toolbelt (from roboflow)\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting filetype (from roboflow)\n",
      "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\tvipi\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\tvipi\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\tvipi\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\tvipi\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\tvipi\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\tvipi\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from torch) (4.13.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\tvipi\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\tvipi\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\tvipi\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from torch) (2025.3.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\tvipi\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\tvipi\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\tvipi\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\tvipi\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\tvipi\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.4.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\tvipi\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from tqdm>=4.64.0->ultralytics) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\tvipi\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Downloading ultralytics-8.3.107-py3-none-any.whl (974 kB)\n",
      "   ---------------------------------------- 0.0/974.5 kB ? eta -:--:--\n",
      "   --------------------------------------- 974.5/974.5 kB 23.0 MB/s eta 0:00:00\n",
      "Downloading roboflow-1.1.61-py3-none-any.whl (85 kB)\n",
      "Downloading idna-3.7-py3-none-any.whl (66 kB)\n",
      "Downloading opencv_python_headless-4.10.0.84-cp37-abi3-win_amd64.whl (38.8 MB)\n",
      "   ---------------------------------------- 0.0/38.8 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 10.5/38.8 MB 54.7 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 10.5/38.8 MB 54.7 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 22.8/38.8 MB 37.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 35.4/38.8 MB 42.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 38.8/38.8 MB 43.2 MB/s eta 0:00:00\n",
      "Downloading pillow_heif-0.22.0-cp310-cp310-win_amd64.whl (8.6 MB)\n",
      "   ---------------------------------------- 0.0/8.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 8.6/8.6 MB 53.2 MB/s eta 0:00:00\n",
      "Downloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\n",
      "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
      "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Installing collected packages: py-cpuinfo, filetype, python-dotenv, pillow-heif, opencv-python-headless, idna, ultralytics-thop, requests-toolbelt, ultralytics, roboflow\n",
      "Successfully installed filetype-1.2.0 idna-3.7 opencv-python-headless-4.10.0.84 pillow-heif-0.22.0 py-cpuinfo-9.0.0 python-dotenv-1.1.0 requests-toolbelt-1.0.0 roboflow-1.1.61 ultralytics-8.3.107 ultralytics-thop-2.0.14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script cpuinfo.exe is installed in 'C:\\Users\\tvipi\\AppData\\Roaming\\Python\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script filetype.exe is installed in 'C:\\Users\\tvipi\\AppData\\Roaming\\Python\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script dotenv.exe is installed in 'C:\\Users\\tvipi\\AppData\\Roaming\\Python\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts ultralytics.exe and yolo.exe are installed in 'C:\\Users\\tvipi\\AppData\\Roaming\\Python\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script roboflow.exe is installed in 'C:\\Users\\tvipi\\AppData\\Roaming\\Python\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "# !pip install --user ultralytics roboflow opencv-python matplotlib torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "71A6tkIhXUMP",
    "outputId": "14e3e8ab-e42e-45b1-8a33-ccf0d9ab19e0"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from roboflow import Roboflow\n",
    "from ultralytics import YOLO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a5CxNXXzXWvR",
    "outputId": "e7662d21-11f9-4fd4-fd04-a1312e94234c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rf = Roboflow(api_key=\"GxUm6xj9W0awEDr6BHuY\")\n",
    "project = rf.workspace(\"weopon-detection\").project(\"weapon-detection-using-yolov8\")\n",
    "version = project.version(1)\n",
    "dataset = version.download(\"yolov8\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sfAKKhu9ygOi"
   },
   "source": [
    "# Train model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dxJczQn4dypF",
    "outputId": "57683712-3e73-40e0-b93c-7e0ef872aade"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8m.pt to 'yolov8m.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 49.7M/49.7M [00:38<00:00, 1.35MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained YOLOv8s model\n",
    "model = YOLO(\"yolov8m.pt\")  # Using YOLOv8 Small for speed\n",
    "\n",
    "# Train model on our weapon detection dataset\n",
    "# model.train(data=f\"{dataset.location}/data.yaml\", epochs=10, imgsz=640)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.107  Python-3.10.16 torch-2.6.0+cu126 CUDA:0 (NVIDIA GeForce RTX 4060 Laptop GPU, 8188MiB)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolov8m.pt, data=C:\\Users\\tvipi\\project\\Weapon-Detection-YOLO\\Weapon-Detection-using-YOLOv8-1/data.yaml, epochs=30, time=None, patience=10, batch=4, imgsz=512, save=True, save_period=-1, cache=False, device=None, workers=4, project=None, name=train6, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=True, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.001, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=True, mixup=0.2, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train6\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1392  ultralytics.nn.modules.conv.Conv             [3, 48, 3, 2]                 \n",
      "  1                  -1  1     41664  ultralytics.nn.modules.conv.Conv             [48, 96, 3, 2]                \n",
      "  2                  -1  2    111360  ultralytics.nn.modules.block.C2f             [96, 96, 2, True]             \n",
      "  3                  -1  1    166272  ultralytics.nn.modules.conv.Conv             [96, 192, 3, 2]               \n",
      "  4                  -1  4    813312  ultralytics.nn.modules.block.C2f             [192, 192, 4, True]           \n",
      "  5                  -1  1    664320  ultralytics.nn.modules.conv.Conv             [192, 384, 3, 2]              \n",
      "  6                  -1  4   3248640  ultralytics.nn.modules.block.C2f             [384, 384, 4, True]           \n",
      "  7                  -1  1   1991808  ultralytics.nn.modules.conv.Conv             [384, 576, 3, 2]              \n",
      "  8                  -1  2   3985920  ultralytics.nn.modules.block.C2f             [576, 576, 2, True]           \n",
      "  9                  -1  1    831168  ultralytics.nn.modules.block.SPPF            [576, 576, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  2   1993728  ultralytics.nn.modules.block.C2f             [960, 384, 2]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  2    517632  ultralytics.nn.modules.block.C2f             [576, 192, 2]                 \n",
      " 16                  -1  1    332160  ultralytics.nn.modules.conv.Conv             [192, 192, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  2   1846272  ultralytics.nn.modules.block.C2f             [576, 384, 2]                 \n",
      " 19                  -1  1   1327872  ultralytics.nn.modules.conv.Conv             [384, 384, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  2   4207104  ultralytics.nn.modules.block.C2f             [960, 576, 2]                 \n",
      " 22        [15, 18, 21]  1   3779749  ultralytics.nn.modules.head.Detect           [7, [192, 384, 576]]          \n",
      "Model summary: 169 layers, 25,860,373 parameters, 25,860,357 gradients, 79.1 GFLOPs\n",
      "\n",
      "Transferred 475/475 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\detect\\train6', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\tvipi\\project\\Weapon-Detection-YOLO\\Weapon-Detection-using-YOLOv8-1\\train\\labels.cache... 470 \u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\tvipi\\project\\Weapon-Detection-YOLO\\Weapon-Detection-using-YOLOv8-1\\valid\\labels.cache... 135 im\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs\\detect\\train6\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.001' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000909, momentum=0.9) with parameter groups 77 weight(decay=0.0), 84 weight(decay=0.0005), 83 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added \n",
      "Image sizes 512 train, 512 val\n",
      "Using 4 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train6\u001b[0m\n",
      "Starting training for 30 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/30      1.92G      1.404      3.161      1.663          9        512: 100%|██████████| 118/118 [00:19<00:00,  \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:02"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        135        180      0.357      0.201      0.126     0.0674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/30      2.18G      1.662      3.097      1.843          5        512: 100%|██████████| 118/118 [00:17<00:00,  \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:01"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        135        180      0.313      0.109     0.0336     0.0142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/30      2.25G       1.87      3.249      2.036          3        512: 100%|██████████| 118/118 [00:16<00:00,  \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:01"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        135        180      0.328     0.0853     0.0423     0.0157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/30      2.39G      1.796       3.21      2.012          5        512: 100%|██████████| 118/118 [00:16<00:00,  \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:01"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        135        180      0.343      0.094     0.0513     0.0162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/30      2.47G      1.819      3.188      2.042          4        512: 100%|██████████| 118/118 [00:16<00:00,  \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:01"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        135        180     0.0292     0.0903     0.0286    0.00917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/30      2.47G      1.833       3.22      2.026          3        512: 100%|██████████| 118/118 [00:16<00:00,  \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:01"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        135        180      0.232     0.0904     0.0681     0.0197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/30      2.47G      1.874      3.228       2.04          8        512: 100%|██████████| 118/118 [00:19<00:00,  \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:01"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        135        180      0.284      0.132     0.0964     0.0316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/30       2.5G      1.823      3.192      2.025         11        512: 100%|██████████| 118/118 [00:16<00:00,  \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:01"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        135        180      0.241      0.275      0.112     0.0462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/30      2.57G      1.726      3.094      1.958         12        512: 100%|██████████| 118/118 [00:16<00:00,  \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:01"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        135        180      0.293      0.188      0.164     0.0717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/30      2.64G      1.729      3.078      1.965          8        512: 100%|██████████| 118/118 [00:16<00:00,  \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:01"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        135        180      0.344      0.225      0.152     0.0624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      11/30      2.78G       1.66      2.973      1.887          8        512: 100%|██████████| 118/118 [00:16<00:00,  \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:02"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        135        180      0.117      0.243      0.111     0.0474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      12/30      2.89G       1.61      2.893      1.854          8        512: 100%|██████████| 118/118 [00:26<00:00,  \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:01"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        135        180      0.238      0.245      0.183     0.0874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      13/30      3.05G      1.555       2.85      1.839          9        512: 100%|██████████| 118/118 [00:16<00:00,  \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:01"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        135        180       0.41      0.213      0.205      0.105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      14/30      3.19G      1.552      2.832      1.836          3        512: 100%|██████████| 118/118 [00:16<00:00,  \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:01"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        135        180      0.416      0.244      0.252      0.124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      15/30      3.36G      1.538      2.825      1.852          9        512: 100%|██████████| 118/118 [00:16<00:00,  \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:01"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        135        180      0.288      0.196      0.208      0.109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      16/30       3.5G       1.52      2.659      1.817          5        512: 100%|██████████| 118/118 [00:16<00:00,  \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:01"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        135        180      0.571      0.274      0.279      0.156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      17/30      3.66G      1.515      2.699      1.819          3        512: 100%|██████████| 118/118 [00:16<00:00,  \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:01"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        135        180      0.358      0.259      0.243      0.117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      18/30      3.81G      1.474      2.612      1.766         17        512: 100%|██████████| 118/118 [00:16<00:00,  \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:01"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        135        180      0.656      0.245       0.31       0.17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      19/30      3.92G       1.47      2.587       1.76         11        512: 100%|██████████| 118/118 [00:16<00:00,  \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:01"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        135        180      0.297      0.351      0.282       0.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      20/30      3.92G       1.38      2.439      1.699          2        512: 100%|██████████| 118/118 [00:16<00:00,  \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:01"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        135        180      0.463      0.334      0.321      0.171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      21/30      3.92G      1.333      2.468      1.701          2        512: 100%|██████████| 118/118 [00:16<00:00,  \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:01"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        135        180      0.389      0.319       0.32      0.173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      22/30      3.92G      1.314      2.253      1.727          4        512: 100%|██████████| 118/118 [00:16<00:00,  \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:01"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        135        180      0.363      0.404       0.33      0.192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      23/30      3.92G      1.275      2.151      1.677          2        512: 100%|██████████| 118/118 [00:16<00:00,  \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:01"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        135        180      0.327      0.407      0.329      0.186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      24/30      3.92G      1.242      2.001       1.63          3        512: 100%|██████████| 118/118 [00:16<00:00,  \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:01"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        135        180      0.415      0.323      0.353      0.206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      25/30      3.92G      1.175      1.923      1.571          6        512: 100%|██████████| 118/118 [00:16<00:00,  \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:01"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        135        180      0.491      0.361      0.388      0.223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      26/30      3.92G      1.127      1.842      1.564          3        512: 100%|██████████| 118/118 [00:16<00:00,  \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:01"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        135        180      0.453      0.376      0.386      0.237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      27/30      3.92G      1.074      1.754      1.499          1        512: 100%|██████████| 118/118 [00:16<00:00,  \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:01"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        135        180       0.47      0.385       0.42       0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      28/30      3.92G      1.084      1.693      1.509          3        512: 100%|██████████| 118/118 [00:16<00:00,  \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:01"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        135        180      0.497      0.367       0.41      0.252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      29/30      3.92G      1.013      1.644      1.432          2        512: 100%|██████████| 118/118 [00:16<00:00,  \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:01"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        135        180      0.428      0.449      0.435      0.272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      30/30      3.92G     0.9922      1.564      1.427          2        512: 100%|██████████| 118/118 [00:17<00:00,  \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:01"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        135        180      0.515      0.416      0.457      0.284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "30 epochs completed in 0.176 hours.\n",
      "Optimizer stripped from runs\\detect\\train6\\weights\\last.pt, 52.0MB\n",
      "Optimizer stripped from runs\\detect\\train6\\weights\\best.pt, 52.0MB\n",
      "\n",
      "Validating runs\\detect\\train6\\weights\\best.pt...\n",
      "Ultralytics 8.3.107  Python-3.10.16 torch-2.6.0+cu126 CUDA:0 (NVIDIA GeForce RTX 4060 Laptop GPU, 8188MiB)\n",
      "Model summary (fused): 92 layers, 25,843,813 parameters, 0 gradients, 78.7 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        135        180      0.493      0.497       0.48      0.304\n",
      "               Handgun         20         20      0.669       0.55      0.551       0.35\n",
      "                 Knife         20         29      0.387      0.379      0.428      0.295\n",
      "               Missile         16         24      0.467        0.5      0.472      0.213\n",
      "                 Rifle         22         30      0.419      0.433      0.309      0.172\n",
      "               Shotgun         15         19      0.546      0.368      0.385      0.234\n",
      "                 Sword         23         26      0.506      0.615      0.629      0.518\n",
      "                  Tank         19         32      0.456       0.63      0.589      0.345\n",
      "Speed: 0.2ms preprocess, 16.3ms inference, 0.0ms loss, 1.5ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train6\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Clear any cached memory from previous runs\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Training the YOLOv8 model\n",
    "model.train(\n",
    "    data=f\"{dataset.location}/data.yaml\",  # from Roboflow\n",
    "    epochs=30,                            # reduced epochs to decrease overall memory usage\n",
    "    imgsz=512,                            # further reduced image resolution to save memory\n",
    "    batch=4,                              # reduced batch size further to fit GPU memory\n",
    "    patience=10,                          # early stop if no validation improvement\n",
    "    val=True,                             # run validation each epoch\n",
    "    augment=True,                         # basic augmentations\n",
    "    mosaic=True,                          # better generalization\n",
    "    mixup=0.2,                            # image blending for robustness\n",
    "    hsv_h=0.015,                          # add color noise\n",
    "    hsv_s=0.7,\n",
    "    hsv_v=0.4,\n",
    "    lr0=0.001,                            # start learning rate\n",
    "    lrf=0.01,                             # final learning rate\n",
    "    weight_decay=0.0005,                  # regularization\n",
    "    momentum=0.937,                       # helps stable training\n",
    "    workers=4,                            # reduced workers for better memory handling\n",
    "    save=True,                            # save best checkpoint\n",
    "    resume=False,                         # not resuming from previous\n",
    "    amp=True,                             # enable mixed-precision training\n",
    ")\n",
    "\n",
    "# Clear CUDA cache after training starts to reduce memory fragmentation\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "GNI_yxbreRJe"
   },
   "outputs": [],
   "source": [
    "model.save(\"weapon_detection_using_YOLOv8-1.pt\")  # Save model to current directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate model and visualize performance\n",
    "metrics = model.val()  # Runs validation on the validation set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Performance Metrics:\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Metric' object has no attribute 'precision'. See valid attributes below.\n\n    Class for computing evaluation metrics for YOLOv8 model.\n\n    Attributes:\n        p (list): Precision for each class. Shape: (nc,).\n        r (list): Recall for each class. Shape: (nc,).\n        f1 (list): F1 score for each class. Shape: (nc,).\n        all_ap (list): AP scores for all classes and all IoU thresholds. Shape: (nc, 10).\n        ap_class_index (list): Index of class for each AP score. Shape: (nc,).\n        nc (int): Number of classes.\n\n    Methods:\n        ap50(): AP at IoU threshold of 0.5 for all classes. Returns: List of AP scores. Shape: (nc,) or [].\n        ap(): AP at IoU thresholds from 0.5 to 0.95 for all classes. Returns: List of AP scores. Shape: (nc,) or [].\n        mp(): Mean precision of all classes. Returns: Float.\n        mr(): Mean recall of all classes. Returns: Float.\n        map50(): Mean AP at IoU threshold of 0.5 for all classes. Returns: Float.\n        map75(): Mean AP at IoU threshold of 0.75 for all classes. Returns: Float.\n        map(): Mean AP at IoU thresholds from 0.5 to 0.95 for all classes. Returns: Float.\n        mean_results(): Mean of results, returns mp, mr, map50, map.\n        class_result(i): Class-aware result, returns p[i], r[i], ap50[i], ap[i].\n        maps(): mAP of each class. Returns: Array of mAP scores, shape: (nc,).\n        fitness(): Model fitness as a weighted combination of metrics. Returns: Float.\n        update(results): Update metric attributes with new evaluation results.\n    ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Print performance stats\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m📊 Performance Metrics:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrecision: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mbox_metrics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecision\u001b[49m()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecall: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbox_metrics\u001b[38;5;241m.\u001b[39mrecall()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmAP@0.5: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbox_metrics\u001b[38;5;241m.\u001b[39mmap50\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\ultralytics\\utils\\__init__.py:241\u001b[0m, in \u001b[0;36mSimpleClass.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Custom attribute access error message with helpful information.\"\"\"\u001b[39;00m\n\u001b[0;32m    240\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m--> 241\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. See valid attributes below.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Metric' object has no attribute 'precision'. See valid attributes below.\n\n    Class for computing evaluation metrics for YOLOv8 model.\n\n    Attributes:\n        p (list): Precision for each class. Shape: (nc,).\n        r (list): Recall for each class. Shape: (nc,).\n        f1 (list): F1 score for each class. Shape: (nc,).\n        all_ap (list): AP scores for all classes and all IoU thresholds. Shape: (nc, 10).\n        ap_class_index (list): Index of class for each AP score. Shape: (nc,).\n        nc (int): Number of classes.\n\n    Methods:\n        ap50(): AP at IoU threshold of 0.5 for all classes. Returns: List of AP scores. Shape: (nc,) or [].\n        ap(): AP at IoU thresholds from 0.5 to 0.95 for all classes. Returns: List of AP scores. Shape: (nc,) or [].\n        mp(): Mean precision of all classes. Returns: Float.\n        mr(): Mean recall of all classes. Returns: Float.\n        map50(): Mean AP at IoU threshold of 0.5 for all classes. Returns: Float.\n        map75(): Mean AP at IoU threshold of 0.75 for all classes. Returns: Float.\n        map(): Mean AP at IoU thresholds from 0.5 to 0.95 for all classes. Returns: Float.\n        mean_results(): Mean of results, returns mp, mr, map50, map.\n        class_result(i): Class-aware result, returns p[i], r[i], ap50[i], ap[i].\n        maps(): mAP of each class. Returns: Array of mAP scores, shape: (nc,).\n        fitness(): Model fitness as a weighted combination of metrics. Returns: Float.\n        update(results): Update metric attributes with new evaluation results.\n    "
     ]
    }
   ],
   "source": [
    "# Access metrics via .box (which is a Metric object)\n",
    "box_metrics = metrics.box\n",
    "\n",
    "# Print performance stats\n",
    "print(\"📊 Performance Metrics:\")\n",
    "print(f\"Precision: {box_metrics.precision():.3f}\")\n",
    "print(f\"Recall: {box_metrics.recall():.3f}\")\n",
    "print(f\"mAP@0.5: {box_metrics.map50:.3f}\")\n",
    "print(f\"mAP@0.5:0.95: {box_metrics.map:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Show where to find visualizations\n",
    "import os\n",
    "from glob import glob\n",
    "from IPython.display import Image, display\n",
    "\n",
    "val_dir = sorted(glob(\"runs/val/exp*/\"))[-1]  # get latest val run\n",
    "\n",
    "print(f\"\\n📂 Visual results saved in: {val_dir}\")\n",
    "\n",
    "# Display common result images\n",
    "images_to_show = [\n",
    "    \"confusion_matrix.png\",\n",
    "    \"PR_curves.png\",\n",
    "    \"F1_curve.png\",\n",
    "    \"labels.jpg\",\n",
    "    \"predictions.jpg\",\n",
    "    \"results.png\"\n",
    "]\n",
    "\n",
    "for img_name in images_to_show:\n",
    "    img_path = os.path.join(val_dir, img_name)\n",
    "    if os.path.exists(img_path):\n",
    "        print(f\"📷 Showing: {img_name}\")\n",
    "        display(Image(filename=img_path))\n",
    "    else:\n",
    "        print(f\"⚠️ Not found: {img_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YfVxB8fmynk7"
   },
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FRCONDWJPFlA",
    "outputId": "8ff4405f-bb93-47b4-c716-238a7852f33a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ultralytics in /usr/local/lib/python3.11/dist-packages (8.3.81)\n",
      "Requirement already satisfied: numpy<=2.1.1,>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.26.4)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.10.0)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.11.0.86)\n",
      "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.1.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.13.1)\n",
      "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.5.1+cu124)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.20.1+cu124)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (5.9.5)\n",
      "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\n",
      "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.2)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.13.2)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.0.14)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.1.31)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "# !pip install ultralytics\n",
    "\n",
    "# from ultralytics import YOLO\n",
    "\n",
    "# model = YOLO(\"model_weapon_detection_using_YOLOv8-1.pt\")  # Change this to your actual model path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "6VbLxMUMXLHY"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "# from ultralytics import YOLO\n",
    "\n",
    "def check_result(test_img):\n",
    "  # Convert BGR to RGB (for correct display)\n",
    "  test_img_rgb = cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB)\n",
    "  # model = YOLO('/content/model_weapon_detection_using_YOLOv8-1.pt')\n",
    "  # Run YOLOv8 detection\n",
    "  results = model(test_img_rgb)\n",
    "\n",
    "# Plot the output\n",
    "  annotated_img = results[0].plot()\n",
    "\n",
    "# Show the image\n",
    "  plt.figure(figsize=(8,8))\n",
    "  plt.imshow(annotated_img)\n",
    "  plt.axis(\"off\")\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: C:\\Users\\tvipi\\project\\Weapon-Detection-YOLO\n",
      "File exists: False\n",
      "Absolute path: C:\\Users\\tvipi\\project\\Weapon-Detection-YOLO\\project\\Weapon-Detection-YOLO\\testimage.jpg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define path\n",
    "test_img_path = \"project/Weapon-Detection-YOLO/testimage.jpg\"\n",
    "\n",
    "# Check if file exists\n",
    "print(\"Current Working Directory:\", os.getcwd())\n",
    "print(\"File exists:\", os.path.exists(test_img_path))\n",
    "print(\"Absolute path:\", os.path.abspath(test_img_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: matplotlib in c:\\users\\tvipi\\anaconda3\\envs\\gpu_env\\lib\\site-packages (3.10.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\tvipi\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\tvipi\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\tvipi\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\tvipi\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\tvipi\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\tvipi\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\tvipi\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\tvipi\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\tvipi\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\tvipi\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "lajZxlJpSafG"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = cv2.imread(\"testimage.jpg\")\n",
    "img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.imshow(img_rgb)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Input Image\")\n",
    "plt.show(block=True)  # 👈 this forces it to pop up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x-DV_VlHY3ej",
    "outputId": "acabc1d2-5b22-465c-de58-af7f2b015fac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[121, 121, 121],\n",
       "        [127, 127, 127],\n",
       "        [136, 136, 136],\n",
       "        ...,\n",
       "        [207, 207, 207],\n",
       "        [207, 207, 207],\n",
       "        [207, 207, 207]],\n",
       "\n",
       "       [[140, 140, 140],\n",
       "        [144, 144, 144],\n",
       "        [152, 152, 152],\n",
       "        ...,\n",
       "        [207, 207, 207],\n",
       "        [207, 207, 207],\n",
       "        [207, 207, 207]],\n",
       "\n",
       "       [[160, 160, 160],\n",
       "        [163, 163, 163],\n",
       "        [168, 168, 168],\n",
       "        ...,\n",
       "        [207, 207, 207],\n",
       "        [207, 207, 207],\n",
       "        [207, 207, 207]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[131, 131, 131],\n",
       "        [131, 131, 131],\n",
       "        [131, 131, 131],\n",
       "        ...,\n",
       "        [177, 177, 177],\n",
       "        [177, 177, 177],\n",
       "        [177, 177, 177]],\n",
       "\n",
       "       [[131, 131, 131],\n",
       "        [131, 131, 131],\n",
       "        [131, 131, 131],\n",
       "        ...,\n",
       "        [177, 177, 177],\n",
       "        [177, 177, 177],\n",
       "        [177, 177, 177]],\n",
       "\n",
       "       [[131, 131, 131],\n",
       "        [131, 131, 131],\n",
       "        [131, 131, 131],\n",
       "        ...,\n",
       "        [177, 177, 177],\n",
       "        [177, 177, 177],\n",
       "        [177, 177, 177]]], dtype=uint8)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 680
    },
    "id": "DeqayV_PYXra",
    "outputId": "a4b5b9c0-b3d5-45dd-e97a-dd6fe9ea8cdb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[125, 125, 125],\n",
       "        [143, 143, 143],\n",
       "        [162, 162, 162],\n",
       "        ...,\n",
       "        [207, 207, 207],\n",
       "        [207, 207, 207],\n",
       "        [207, 207, 207]],\n",
       "\n",
       "       [[146, 146, 146],\n",
       "        [160, 160, 160],\n",
       "        [173, 173, 173],\n",
       "        ...,\n",
       "        [207, 207, 207],\n",
       "        [207, 207, 207],\n",
       "        [207, 207, 207]],\n",
       "\n",
       "       [[166, 166, 166],\n",
       "        [174, 174, 174],\n",
       "        [181, 181, 181],\n",
       "        ...,\n",
       "        [207, 207, 207],\n",
       "        [207, 207, 207],\n",
       "        [207, 207, 207]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[131, 131, 131],\n",
       "        [131, 131, 131],\n",
       "        [131, 131, 131],\n",
       "        ...,\n",
       "        [177, 177, 177],\n",
       "        [177, 177, 177],\n",
       "        [177, 177, 177]],\n",
       "\n",
       "       [[131, 131, 131],\n",
       "        [131, 131, 131],\n",
       "        [131, 131, 131],\n",
       "        ...,\n",
       "        [177, 177, 177],\n",
       "        [177, 177, 177],\n",
       "        [177, 177, 177]],\n",
       "\n",
       "       [[131, 131, 131],\n",
       "        [131, 131, 131],\n",
       "        [131, 131, 131],\n",
       "        ...,\n",
       "        [177, 177, 177],\n",
       "        [177, 177, 177],\n",
       "        [177, 177, 177]]], dtype=uint8)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_img = cv2.resize(test_img, (640,640))\n",
    "test_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 708
    },
    "id": "0VRFPUVgYEbo",
    "outputId": "99c6f71c-5e28-48eb-9d44-83089e369fb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 512x512 (no detections), 235.3ms\n",
      "Speed: 4.2ms preprocess, 235.3ms inference, 1.0ms postprocess per image at shape (1, 3, 512, 512)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "check_result(test_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p6TB_tGcy5eN"
   },
   "source": [
    "# **Live Detection**  \n",
    "\n",
    "**Note:** In my case, I ran this on **Jupyter Notebook**. This code **failed to open the camera in Colab**, so I executed it in **Jupyter Notebook instead**.  \n",
    "\n",
    "### **Final Steps:**  \n",
    "1️⃣ **Opened the webcam** using OpenCV.  \n",
    "2️⃣ **Captured a photo** from the live feed.  \n",
    "3️⃣ **Processed the image** using the trained YOLO model.  \n",
    "4️⃣ **Detected weapons**, if present, and drew bounding boxes.  \n",
    "5️⃣ **Displayed two images**:  \n",
    "   - **Original Image** (captured from the camera).  \n",
    "   - **Processed Image** (with detected weapons and bounding boxes).  \n",
    "\n",
    "✅ **Result:** The system successfully detected weapons in real time and displayed the output. 🚀  \n",
    "\n",
    "---\n",
    "\n",
    "This should be good for documentation! Let me know if you need any edits. 😊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yfr8ETBqS79O"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the trained YOLO model\n",
    "model = YOLO(\"model_weapon_detection_using_YOLOv8-1.pt\")  # Change this to your actual model path\n",
    "\n",
    "def live_detection():\n",
    "    # Open the webcam\n",
    "    cap = cv2.VideoCapture(0)  # Change to 1 if using an external webcam\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open webcam.\")\n",
    "        return\n",
    "\n",
    "    # Capture a single frame\n",
    "    ret, frame = cap.read()\n",
    "    cap.release()  # Close the camera immediately after taking the image\n",
    "\n",
    "    if not ret:\n",
    "        print(\"Error: Could not capture image.\")\n",
    "        return\n",
    "\n",
    "    # Save the captured image\n",
    "    image_path = \"captured_image.jpg\"\n",
    "    cv2.imwrite(image_path, frame)\n",
    "\n",
    "    print(f\"Image captured and saved as {image_path}\")\n",
    "\n",
    "    # Run YOLO detection\n",
    "    results = model(frame)\n",
    "\n",
    "    # Save the detection output\n",
    "    detected_image_path = \"detected_image.jpg\"\n",
    "    results[0].save(filename=detected_image_path)  # Save the detected image\n",
    "\n",
    "    # Display both original and detected images\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    # Show Original Image\n",
    "    original = cv2.imread(image_path)  # Read the saved image\n",
    "    original = cv2.cvtColor(original, cv2.COLOR_BGR2RGB)  # Convert to RGB for correct display\n",
    "    axes[0].imshow(original)\n",
    "    axes[0].set_title(\"Captured Image\")\n",
    "    axes[0].axis(\"off\")\n",
    "\n",
    "    # Show Detected Image\n",
    "    detected = cv2.imread(detected_image_path)  # Read detected image\n",
    "    detected = cv2.cvtColor(detected, cv2.COLOR_BGR2RGB)  # Convert to RGB for correct display\n",
    "    axes[1].imshow(detected)\n",
    "    axes[1].set_title(\"Weapon Detection Result\")\n",
    "    axes[1].axis(\"off\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Run the function\n",
    "live_detection()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rCZeZxp_lS20"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:gpu_env]",
   "language": "python",
   "name": "conda-env-gpu_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
